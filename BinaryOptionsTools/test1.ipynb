{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator\n",
    "from ta.trend import MACD, SMAIndicator, EMAIndicator, ADXIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv(r'C:\\Users\\vigop\\BinaryOptionsTools\\history-AUDNZD_otc.csv')\n",
    "# Calculate MACD\n",
    "macd = MACD(close=data['close'])\n",
    "data['macd'] = macd.macd()\n",
    "data['macd_signal'] = macd.macd_signal()\n",
    "\n",
    "# Calculate Moving Averages (SMA and EMA)\n",
    "\n",
    "\n",
    "# Drop any NaN values generated by the indicators\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Define the feature columns, including the new signals\n",
    "features = ['open', 'high', 'low', 'close','macd', 'macd_signal']\n",
    "\n",
    "# Define a future prediction window (e.g., 5 periods ahead)\n",
    "prediction_window = 5\n",
    "\n",
    "# Create the target column: 1 if future close price is higher, else 0\n",
    "data['target'] = (data['close'].shift(-prediction_window) > data['close']).astype(int)\n",
    "\n",
    "# Extract features and target\n",
    "X = data[features].values\n",
    "y = data['target'].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class BinaryOptionsLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BinaryOptionsLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Only last time step's output is needed\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mBinaryOptionsLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Forward propagate LSTM\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Only last time step's output is needed\u001b[39;00m\n\u001b[0;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:909\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    907\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    908\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    910\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = len(features)  # Update input size based on the number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = BinaryOptionsLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'binary_options_lstm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).cpu().numpy()\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions and true labels to binary\n",
    "y_test_binary = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6890\n",
      "Epoch [2/50], Loss: 0.6961\n",
      "Epoch [3/50], Loss: 0.6935\n",
      "Epoch [4/50], Loss: 0.6940\n",
      "Epoch [5/50], Loss: 0.6907\n",
      "Epoch [6/50], Loss: 0.6935\n",
      "Epoch [7/50], Loss: 0.6904\n",
      "Epoch [8/50], Loss: 0.6969\n",
      "Epoch [9/50], Loss: 0.6918\n",
      "Epoch [10/50], Loss: 0.6911\n",
      "Epoch [11/50], Loss: 0.6927\n",
      "Epoch [12/50], Loss: 0.6934\n",
      "Epoch [13/50], Loss: 0.6962\n",
      "Epoch [14/50], Loss: 0.6946\n",
      "Epoch [15/50], Loss: 0.6872\n",
      "Epoch [16/50], Loss: 0.6895\n",
      "Epoch [17/50], Loss: 0.6896\n",
      "Epoch [18/50], Loss: 0.6874\n",
      "Epoch [19/50], Loss: 0.6909\n",
      "Epoch [20/50], Loss: 0.6967\n",
      "Epoch [21/50], Loss: 0.6871\n",
      "Epoch [22/50], Loss: 0.6935\n",
      "Epoch [23/50], Loss: 0.6940\n",
      "Epoch [24/50], Loss: 0.6885\n",
      "Epoch [25/50], Loss: 0.6916\n",
      "Epoch [26/50], Loss: 0.6892\n",
      "Epoch [27/50], Loss: 0.6901\n",
      "Epoch [28/50], Loss: 0.6929\n",
      "Epoch [29/50], Loss: 0.6910\n",
      "Epoch [30/50], Loss: 0.6943\n",
      "Epoch [31/50], Loss: 0.6928\n",
      "Epoch [32/50], Loss: 0.6930\n",
      "Epoch [33/50], Loss: 0.6932\n",
      "Epoch [34/50], Loss: 0.6855\n",
      "Epoch [35/50], Loss: 0.6891\n",
      "Epoch [36/50], Loss: 0.6889\n",
      "Epoch [37/50], Loss: 0.6956\n",
      "Epoch [38/50], Loss: 0.6949\n",
      "Epoch [39/50], Loss: 0.6938\n",
      "Epoch [40/50], Loss: 0.6952\n",
      "Epoch [41/50], Loss: 0.6919\n",
      "Epoch [42/50], Loss: 0.6950\n",
      "Epoch [43/50], Loss: 0.6897\n",
      "Epoch [44/50], Loss: 0.6942\n",
      "Epoch [45/50], Loss: 0.6897\n",
      "Epoch [46/50], Loss: 0.6916\n",
      "Epoch [47/50], Loss: 0.6848\n",
      "Epoch [48/50], Loss: 0.6914\n",
      "Epoch [49/50], Loss: 0.6911\n",
      "Epoch [50/50], Loss: 0.6936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a basic neural network\n",
    "class BinaryOptionsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BinaryOptionsModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 6\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BinaryOptionsModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Accuracy: 51.76%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    print(predicted)\n",
    "    accuracy = (predicted.eq(y_test_tensor).sum().item()) / y_test_tensor.size(0)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to binary_options_model2.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the model\n",
    "model_path = \"binary_options_model2.pth\"\n",
    "\n",
    "# Save the model's state_dict (parameters)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for retraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigop\\AppData\\Local\\Temp\\ipykernel_18244\\3150394980.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"trend.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model architecture (same as before)\n",
    "loaded_model = BinaryOptionsModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Load the saved model parameters\n",
    "loaded_model.load_state_dict(torch.load(\"trend.pth\"))\n",
    "\n",
    "# Set the model to training mode (required if you want to train)\n",
    "loaded_model.train()\n",
    "\n",
    "print(\"Model loaded for retraining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer\n",
    "optimizer = optim.Adam(loaded_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optionally, if you had learning rate schedules, reinitialize them as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.3536\n",
      "Epoch [2/50], Loss: 0.2920\n",
      "Epoch [3/50], Loss: 0.3057\n",
      "Epoch [4/50], Loss: 0.3397\n",
      "Epoch [5/50], Loss: 0.4785\n",
      "Epoch [6/50], Loss: 0.3532\n",
      "Epoch [7/50], Loss: 0.3126\n",
      "Epoch [8/50], Loss: 0.3380\n",
      "Epoch [9/50], Loss: 0.2750\n",
      "Epoch [10/50], Loss: 0.4019\n",
      "Epoch [11/50], Loss: 0.2210\n",
      "Epoch [12/50], Loss: 0.2037\n",
      "Epoch [13/50], Loss: 0.3933\n",
      "Epoch [14/50], Loss: 0.2925\n",
      "Epoch [15/50], Loss: 0.2695\n",
      "Epoch [16/50], Loss: 0.3443\n",
      "Epoch [17/50], Loss: 0.3477\n",
      "Epoch [18/50], Loss: 0.2602\n",
      "Epoch [19/50], Loss: 0.2419\n",
      "Epoch [20/50], Loss: 0.3082\n",
      "Epoch [21/50], Loss: 0.3136\n",
      "Epoch [22/50], Loss: 0.3185\n",
      "Epoch [23/50], Loss: 0.3659\n",
      "Epoch [24/50], Loss: 0.3791\n",
      "Epoch [25/50], Loss: 0.2381\n",
      "Epoch [26/50], Loss: 0.3016\n",
      "Epoch [27/50], Loss: 0.3576\n",
      "Epoch [28/50], Loss: 0.2577\n",
      "Epoch [29/50], Loss: 0.3079\n",
      "Epoch [30/50], Loss: 0.3470\n",
      "Epoch [31/50], Loss: 0.3742\n",
      "Epoch [32/50], Loss: 0.2899\n",
      "Epoch [33/50], Loss: 0.3656\n",
      "Epoch [34/50], Loss: 0.3813\n",
      "Epoch [35/50], Loss: 0.2937\n",
      "Epoch [36/50], Loss: 0.3950\n",
      "Epoch [37/50], Loss: 0.2236\n",
      "Epoch [38/50], Loss: 0.2722\n",
      "Epoch [39/50], Loss: 0.3692\n",
      "Epoch [40/50], Loss: 0.2650\n",
      "Epoch [41/50], Loss: 0.3256\n",
      "Epoch [42/50], Loss: 0.2313\n",
      "Epoch [43/50], Loss: 0.3300\n",
      "Epoch [44/50], Loss: 0.3124\n",
      "Epoch [45/50], Loss: 0.2513\n",
      "Epoch [46/50], Loss: 0.2941\n",
      "Epoch [47/50], Loss: 0.2620\n",
      "Epoch [48/50], Loss: 0.3156\n",
      "Epoch [49/50], Loss: 0.2865\n",
      "Epoch [50/50], Loss: 0.2477\n",
      "Epoch [51/50], Loss: 0.3507\n",
      "Epoch [52/50], Loss: 0.2586\n",
      "Epoch [53/50], Loss: 0.2525\n",
      "Epoch [54/50], Loss: 0.3107\n",
      "Epoch [55/50], Loss: 0.4007\n",
      "Epoch [56/50], Loss: 0.2857\n",
      "Epoch [57/50], Loss: 0.2377\n",
      "Epoch [58/50], Loss: 0.3739\n",
      "Epoch [59/50], Loss: 0.2552\n",
      "Epoch [60/50], Loss: 0.2583\n",
      "Epoch [61/50], Loss: 0.4168\n",
      "Epoch [62/50], Loss: 0.2175\n",
      "Epoch [63/50], Loss: 0.2819\n",
      "Epoch [64/50], Loss: 0.2630\n",
      "Epoch [65/50], Loss: 0.3749\n",
      "Epoch [66/50], Loss: 0.3385\n",
      "Epoch [67/50], Loss: 0.3152\n",
      "Epoch [68/50], Loss: 0.3000\n",
      "Epoch [69/50], Loss: 0.2303\n",
      "Epoch [70/50], Loss: 0.3435\n",
      "Epoch [71/50], Loss: 0.2848\n",
      "Epoch [72/50], Loss: 0.4285\n",
      "Epoch [73/50], Loss: 0.2897\n",
      "Epoch [74/50], Loss: 0.2086\n",
      "Epoch [75/50], Loss: 0.3223\n",
      "Epoch [76/50], Loss: 0.2647\n",
      "Epoch [77/50], Loss: 0.3430\n",
      "Epoch [78/50], Loss: 0.4288\n",
      "Epoch [79/50], Loss: 0.3369\n",
      "Epoch [80/50], Loss: 0.3186\n",
      "Epoch [81/50], Loss: 0.2711\n",
      "Epoch [82/50], Loss: 0.2874\n",
      "Epoch [83/50], Loss: 0.4217\n",
      "Epoch [84/50], Loss: 0.1922\n",
      "Epoch [85/50], Loss: 0.2601\n",
      "Epoch [86/50], Loss: 0.3579\n",
      "Epoch [87/50], Loss: 0.3196\n",
      "Epoch [88/50], Loss: 0.1893\n",
      "Epoch [89/50], Loss: 0.2911\n",
      "Epoch [90/50], Loss: 0.2027\n",
      "Epoch [91/50], Loss: 0.3421\n",
      "Epoch [92/50], Loss: 0.2961\n",
      "Epoch [93/50], Loss: 0.2234\n",
      "Epoch [94/50], Loss: 0.3235\n",
      "Epoch [95/50], Loss: 0.3415\n",
      "Epoch [96/50], Loss: 0.2929\n",
      "Epoch [97/50], Loss: 0.3611\n",
      "Epoch [98/50], Loss: 0.2836\n",
      "Epoch [99/50], Loss: 0.2913\n",
      "Epoch [100/50], Loss: 0.3859\n"
     ]
    }
   ],
   "source": [
    "# Training loop (same as before)\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = loaded_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained model saved\n"
     ]
    }
   ],
   "source": [
    "# Save the retrained model\n",
    "torch.save(loaded_model.state_dict(), \"binary_options_model_retrained2.pth\")\n",
    "print(\"Retrained model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "Accuracy: 67.61%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(X_test_tensor)\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    print(predicted)\n",
    "    accuracy = (predicted.eq(y_test_tensor).sum().item()) / y_test_tensor.size(0)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4673"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
